{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./dataset/winequality_red.csv', encoding='utf_8_sig')\n",
    "\n",
    "label = dataset.iloc[:,-1]\n",
    "dataset = dataset.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "통계학에서, 선형 회귀(linear regression)는 종속 변수 y와 한 개 이상의 독립 변수 (또는 설명 변수) X와의 선형 상관 관계를 모델링하는 회귀분석 기법이다. 한 개의 설명 변수에 기반한 경우에는 단순 선형 회귀, 둘 이상의 설명 변수에 기반한 경우에는 다중 선형 회귀라고 한다.\n",
    "\n",
    "선형 회귀는 선형 예측 함수를 사용해 회귀식을 모델링하며, 알려지지 않은 파라미터는 데이터로부터 추정한다. 이렇게 만들어진 회귀식을 선형 모델이라고 한다.\n",
    "선형 회귀는 깊이있게 연구되고 널리 사용된 첫 번째 회귀분석 기법이다. 이는 알려지지 않은 파라미터에 대해 선형 관계를 갖는 모델을 세우는 것이, 비선형 관계를 갖는 모델을 세우는 것보다 용이하기 때문이다.여러 사용 사례가 있지만, 대개 아래와 같은 두 가지 분류 중 하나로 요약할 수 있다.\n",
    "\n",
    "값을 예측하는 것이 목적일 경우, 선형 회귀를 사용해 데이터에 적합한 예측 모형을 개발한다. 개발한 선형 회귀식을 사용해 y가 없는 x값에 대해 y를 예측하기 위해 사용할 수 있다.\n",
    "종속 변수 y와 이것과 연관된 독립 변수 X1, ..., Xp가 존재하는 경우에, 선형 회귀 분석을 사용해 Xj와 y의 관계를 정량화할 수 있다. Xj는 y와 전혀 관계가 없을 수도 있고, 추가적인 정보를 제공하는 변수일 수도 있다.\n",
    "일반적으로 최소제곱법(least square method)을 사용해 선형 회귀 모델을 세운다. 최소제곱법 외에 다른 기법으로도 선형 회귀 모델을 세울 수 있다. 손실 함수(loss fuction)를 최소화 하는 방식으로 선형 회귀 모델을 세울 수도 있다. 최소제곱법은 선형 회귀 모델 뿐 아니라, 비선형 회귀 모델에도 적용할 수 있다. 최소제곱법과 선형 회귀는 가깝게 연관되어 있지만, 그렇다고 해서 동의어는 아니다.  \n",
    "출처 : https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "Linear = linear_model.LinearRegression()\n",
    "Linear.fit(X_train,y_train)\n",
    "y_pred = Linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.34704718e-02 -1.09961969e+00 -2.47859776e-01  7.73785620e-03\n",
      " -1.67359251e+00  4.55041815e-03 -3.26389168e-03 -1.42395563e+01\n",
      " -3.19247444e-01  8.12824701e-01  2.91991158e-01]\n",
      "17.96257834335607\n"
     ]
    }
   ],
   "source": [
    "print(Linear.coef_)\n",
    "print(Linear.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 0.35164491095572137\n",
      "mean_squared_errors: 0.41123487174964946\n",
      "r2_score: 0.3513885332517389\n"
     ]
    }
   ],
   "source": [
    "print('explained_variance_score: {}'.format(explained_variance_score(y_test, y_pred)))\n",
    "print('mean_squared_errors: {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "print('r2_score: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizor\n",
    "\n",
    "머신러닝을 잘 학습시킨다는 것은 최적화와 일반화 사이의 줄다리기라고 할 수 있다. 최적화 (optimization)란 훈련 데이터에서 최고의 성능을 얻기 위해 모델을 조정하는 과정을 말한다. 일반화 (generalization)는 학습된 모델이 새로운 데이터에 대해 얼마나 잘 수행하는지를 의미한다.\n",
    "\n",
    "일반적으로 머신러닝을 학습하면서 과소/과대 적합이라는 문제가 발생한다.\n",
    "과소적합 (underfitting)은 훈련 데이터의 손실이 낮아질수록 검증 데이터의 손실도 낮아지는 단계를 말한다.\n",
    "과대적합 (overfitting)은 훈련 데이터의 손실은 낮아지지만 검증 세트의 손실이 증가하게 되는 상황을 말한다. 훈련 데이터에 특화된 패턴을 학습하기 시작했다는 의미이다.\n",
    "\n",
    "과대적합을 방지하기 위해서 가장 좋은 방법은 더 많은 잘 정돈된 훈련 데이터를 모으는 것이다. 이 때 데이터를 더 모으는 것이 불가능할 때도 있다. 이런 상황에서 모델이 훈련 데이터에서 학습할 수 있는 패턴에 제약을 가해 과대적합을 피할 수 있는데, 이를 규제(regularization) 라고 한다.\n",
    "\n",
    "과대적합을 완화하기 위한 방법은 네트워크의 복잡도에 제한을 두어 가중치가 작은 값을 가지도록 제약을 가하는 것이다. 가중치를 작게 만든다는 것은 특정값(outlier 등)의 영향을 적게 받도록 해줌으로써, 결과적으로 일반화에 적합한 특성을 갖게 만드는 것이라 할 수 있다.\n",
    "이를 가중치 규제 (weight regularization)라고 하며, 네트워크의 손실함수에 큰 가중치에 연관된 비용을 추가하는 데, 두 가지 형태의 비용이 있다.\n",
    "L1 규제 : 가중치의 절대값에 비례하는 비용이 추가된다(가중치의 L1 Norm). \n",
    "L2 규제 : 가중치의 제곱에 비례하는 비용이 추가된다(가중치의 L2 Norm).\n",
    "\n",
    "앞서 말한 것과 같이 과대적합을 방지하기 위해 기존의 손실함수에서 정규화항을 추가하게 되는데, 이를 가중치 규제라고 한다. 이 때 추가되는 항에 따라 회귀 모형의 종류가 달라진다.\n",
    "\n",
    "그럼 언제 어떤 모델을 사용해야할까?\n",
    "일반적으로는 규제항을 추가한 모델이 그렇지 않은 모델보다는 높은 성능을 보이고 있다. Ridge가 기본으로 쓰이고 있지만, 실제로 쓰이는 특성이 적다고 판단되는 경우에는 Lasso나 Elastic Net 모형이 일반적으로 더 많이 쓰입니다. 이는 앞서 말한것과 같이 불필요한 가중치를 0으로 만들어주는 것과 연관되어 있습니다. 또한 특성 수가 훈련샘플 수보다 많거나, 특성 몇 개가 강한 연관관계를 보일때에는 Elastic Net 모형을 선호한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "Lasso Regression의 경우 규제항 자리에 가중치에 대한 L1 Norm을 추가한 선형회귀 버전이다. 특징은 덜 중요한 특성의 가중치를 완전히 제거하려고 한다는 점이다. 즉 다시 말해 자동으로 특성을 선택하고, 희소모델을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "Lasso = linear_model.Lasso(alpha=0.1)\n",
    "Lasso.fit(X_train,y_train)\n",
    "y_pred = Lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01752463 -0.          0.          0.         -0.          0.00632277\n",
      " -0.00412139 -0.         -0.          0.          0.26512127]\n",
      "2.80741030175525\n"
     ]
    }
   ],
   "source": [
    "print(Lasso.coef_)\n",
    "print(Lasso.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 0.21084461308434832\n",
      "mean_squared_errors: 0.5015893646134156\n",
      "r2_score: 0.20887882853160988\n"
     ]
    }
   ],
   "source": [
    "print('explained_variance_score: {}'.format(explained_variance_score(y_test, y_pred)))\n",
    "print('mean_squared_errors: {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "print('r2_score: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge Regression의 경우 규제항 자리에 가중치에 대한 L2 Norm을 추가한 선형회귀 버전이다. alpha값을 증가시킬수록 모델의 분산은 커지고, 감소시킬수록 모델의 분산은 줄지만 편향을 커지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "Ridge = linear_model.Ridge(alpha=0.1)\n",
    "Ridge.fit(X_train,y_train)\n",
    "y_pred = Ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.12502795e-02 -1.11133625e+00 -2.52900014e-01  1.47211579e-03\n",
      " -1.60012884e+00  4.68812574e-03 -3.28105780e-03 -9.47405448e-02\n",
      " -3.80414071e-01  7.82195581e-01  3.05794163e-01]\n",
      "4.059738852904075\n"
     ]
    }
   ],
   "source": [
    "print(Ridge.coef_)\n",
    "print(Ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explained_variance_score: 0.35080842180499405\n",
      "mean_squared_errors: 0.41178016754425534\n",
      "r2_score: 0.35052847704189904\n"
     ]
    }
   ],
   "source": [
    "print('explained_variance_score: {}'.format(explained_variance_score(y_test, y_pred)))\n",
    "print('mean_squared_errors: {}'.format(mean_squared_error(y_test, y_pred)))\n",
    "print('r2_score: {}'.format(r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction Method\n",
    "- Wrapper Method\n",
    "1. Forward Selection\n",
    "2. Backward Selection\n",
    "3. Stepwise Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Forward Selection\n",
    "\n",
    "모델에 변수를 하나씩 추가해 나가면서 best subset을 찾아가는 방법이다. 이 모델에서는 한번 추가된 변수는 제거가 불가능하다는 단점이 있다. 먼저 추가된 변수에 의해 best model을 놓치는 경우가 발생할 수 있으며, 변수가 모델에 추가되는 순서가 중요한 방식이다.\n",
    "\n",
    "[Process]  \n",
    "  1. Model 상에서 boundary로 정할 Significance level을 정한다.\n",
    "  2. 각각의 변수별로 Simple Linear Regression을 수행한다. 이 때 Simple Linear Regression이란 각 변수 하나만 놓고 단항으로 Linear Regression을 수행하라는 것이다.\n",
    "  3. 각 변수에 대한 p-value를 비교한 후, p-value가 가장 낮은 변수를 선정한다. \n",
    "  4. 해당 변수를 놔둔 상태에서 나머지 변수들을 추가하면서 다시 학습을 시킨다. 이때 p-value가 SL보다 높을 경우 해당 과정을 종료한다. 이 때 종료한 상태는 이미 p-value가 높은 변수가 반영되어 있는 모델이므로 해당 변수를 학습시키기 이전 model을 선택한다.\n",
    "  5. 변수를 하나씩 추가하면서 다시 학습시키고 4번 과정을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(data, target, significance_level=0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(list(initial_features))>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<significance_level):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gene\\Anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'volatile acidity',\n",
       " 'sulphates',\n",
       " 'total sulfur dioxide',\n",
       " 'chlorides',\n",
       " 'pH',\n",
       " 'free sulfur dioxide']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_selection(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Backward Elimination\n",
    "\n",
    "forward selection과 다르게 모델에 변수를 하나씩 제거해나가면서 best subset을 찾아가는 방법이다. 동일한 원리로 모델에서 한번 제거된 변수는 다시 추가가 불가능하다.\n",
    "\n",
    "[Process]\n",
    "  1. Model 상에서 boundary로 정할 Significance level(유의수준)을 정한다. (예 SL = 0.05) 이 Significance level(SL)은 보통 우리가 뉴스에서 나오는 설문조사에서 꼭 나오는 p-value(신뢰도)와 반대되는 개념이다. (p-value = 1 - SL) 다시 말해 우리가 정한 SL보다 높은 수치가 나온다는 말은 그만큼 신뢰할 수 없다는 뜻으로 해석할 수 있다.\n",
    "  2. 모든 변수들을 넣고 학습을 시킨후 각 변수별 p-value를 비교한다. 이중 p-value가 가장 높은 변수를 선정한다.\n",
    "  3. 해당 변수의 p-value가 SL보다 높은 경우 해당 변수를 제거한다. 만약 남아있는 모든 변수들의 p-value가 SL보다 낮을 경우 해당 과정을 종료한다.\n",
    "  4. 변수를 제거한 상태에서 다시 학습을 시키고 2번 과정을 다시 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_elimination(data, target,significance_level = 0.05):\n",
    "    features = data.columns.tolist()\n",
    "    while(len(features)>0):\n",
    "        features_with_constant = sm.add_constant(data[features])\n",
    "        p_values = sm.OLS(target, features_with_constant).fit().pvalues[1:]\n",
    "        max_p_value = p_values.max()\n",
    "        if(max_p_value >= significance_level):\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['volatile acidity',\n",
       " 'chlorides',\n",
       " 'free sulfur dioxide',\n",
       " 'total sulfur dioxide',\n",
       " 'pH',\n",
       " 'sulphates',\n",
       " 'alcohol']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backward_elimination(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stepwise Selection\n",
    "\n",
    "단계별 선택법은 forward와 backward의 혼합 버전이다. 각각이 가지고 있는 단점을 보완하기 때문에 유용하게 사용되는 방법이다.\n",
    "\n",
    "[Process]\n",
    "  1. 변수를 넣거나 제거할 때 boundary로 사용할 Significance level을 정한다. (SL_Enter, SL_Stay)\n",
    "  2. Forward Selection을 수행해서 변수를 선정한다.\n",
    "  3. Backward Selection을 수행해서 선정된 변수 중 유의미한 변수만 남기고 제거한다.\n",
    "  그리고 2번 과정을 다시 수행한다.\n",
    "  4. 변수가 추가되거나 제거할 케이스가 없는 경우 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(data, target,SL_in=0.05,SL_out = 0.05):\n",
    "    initial_features = data.columns.tolist()\n",
    "    best_features = []\n",
    "    while (len(list(initial_features))>0):\n",
    "        remaining_features = list(set(initial_features)-set(best_features))\n",
    "        new_pval = pd.Series(index=remaining_features)\n",
    "        for new_column in remaining_features:\n",
    "            model = sm.OLS(target, sm.add_constant(data[best_features+[new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        min_p_value = new_pval.min()\n",
    "        if(min_p_value<SL_in):\n",
    "            best_features.append(new_pval.idxmin())\n",
    "            while(len(best_features)>0):\n",
    "                best_features_with_constant = sm.add_constant(data[best_features])\n",
    "                p_values = sm.OLS(target, best_features_with_constant).fit().pvalues[1:]\n",
    "                max_p_value = p_values.max()\n",
    "                if(max_p_value >= SL_out):\n",
    "                    excluded_feature = p_values.idxmax()\n",
    "                    best_features.remove(excluded_feature)\n",
    "                else:\n",
    "                    break \n",
    "        else:\n",
    "            break\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'volatile acidity',\n",
       " 'sulphates',\n",
       " 'total sulfur dioxide',\n",
       " 'chlorides',\n",
       " 'pH',\n",
       " 'free sulfur dioxide']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stepwise_selection(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialFeatureSelector(clone_estimator=True, cv=0,\n",
       "                          estimator=LinearRegression(copy_X=True,\n",
       "                                                     fit_intercept=True,\n",
       "                                                     n_jobs=None,\n",
       "                                                     normalize=False),\n",
       "                          floating=False, forward=True, k_features=(3, 11),\n",
       "                          n_jobs=1, pre_dispatch='2*n_jobs', scoring=None,\n",
       "                          verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sfs1 = SFS(LinearRegression(), \n",
    "          k_features=(3,11), \n",
    "          forward=True, \n",
    "          floating=False,\n",
    "          cv=0)\n",
    "sfs1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "import matplotlib.pyplot as plt\n",
    "fig1 = plot_sfs(sfs1.get_metric_dict(), kind='std_dev')\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
